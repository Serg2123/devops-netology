1.  
Так как операция все еще выполняется и ее нужно завершить, то будем искать незавершенные операции, которые выполняюся более 3-х минут:  
```
db.currentOp({"active" : true, "secs_running" : { "$gt" : 180 }})
```
далее прервёрм данную операцию:  
```
db.killOP(opid)
opid - код того самого запроса который выполняется более 3 мин, полученный из db.currentOp.
```
Чтобы посмотреть и подумать что можно сделать с данным долгим запросом будем использовать метод db.collection.explain().  
К найденному (через db.currentOp) проблемному запросу допишем либо в конец .explain("executionStats") либо как в примерах в документации:  
db.имя_коллекции.explain("executionStats").оператор_действия_проблемного_запроса(набор_параметров_проблемного_запроса),  
далее соберем статистику и будем смотреть что попадает в индексы, а что нет, какие индексы нужно создать, как можно разделить данные или сам запрос и т.д.  
  
2.  
Т.к. мы активно увеличиваем реплики сервиса которые работают с Redis и видим достаточно разнообразные проблемы:  
- сначала рост отношения записанных значений к истекшим;  
- Redis блокирует операции записи;  
то в итоге проблем может быть даже несколько, которые наслаиваются друг на друга.  
   a. Сначала проверим самое типовое по чеклисту (всякие hugepage, медленные команды и задержки) из офф. документации: https://redis.io/topics/latency;  
   b. Т.к. у нас при росте реплик сервиса скорее всего будут рости коннекты проверис соотвествие разрешенных коннектов в ОС ( /proc/sys/net/core/somaxconn) и настроенных коннектов в Redis "tcp-backlog" в /etc/redis.conf, выставим somaxconn больше чем tcp-backlog;  
   с. Проверим и сравним максимальное количество одновременно подключенных клиентов и лимиты операционной системы на максимальное количество файловых дескрипторов (maxclients и fs.file-max);  
   d. Память:  
     d.1 командой info memory посмотрим что у нас вообще творится с памятью;  
     d.2 узнаем делаем ли мы снепшоты на которые нужно 2Х памяти в системе от занятой Redis (но там ошибка обычно другая вылазит);  
     d.3 узнаем как настроен размер оперативной памяти (maxmemory) выделяемой для кэширования, а так же политика maxmemory-policy.  
     В случае дефолтного (нулевого) значения maxmemory, Redis будет использовать всю доступную оперативную память,  
     в случае выставления конкретного значения и при достижении этого конкретного значения Redis будет сначала пытаться удалять ключи из кеша согласно политике,указанной параметром maxmemory-policy (возможно как раз то что мы видим с изменненым отношением добавленных ключенй и "протухших" ключей).  
     Если Redis не может удалить ключи согласно политике, либо установлена политика ‘noeviction'(политика,при которой ничего не удаляется из кеша, а отдается ошибка при операции записи - скорее всего наш вариант!), то Redis будет отдавать ошибку командам, которые пытались использовать больше памяти (SET,LPUSH и т.д.) и при этом корректно выполнять команды на чтение (GET и т.д.).  
Итого, скорее всего проблема в некорректной настройке политики maxmemory-policy.  
  
3.  
Т.к. проблема начала всплывать именно при росте количества записей в БД и именно при селекте, то можно предположить, что либо этот селект вытягивает все больше и больше данных для пользователя (а так как это гис система, то данных теоретически м.б. много), либо этот селект все дольше и дольше обрабатывает что-то на стороне сервера.  
Тем не менее, основная документация по MySQL проблемы "Lost connect" относит к сетевым проблемам, и рекомендует их решать по следующему алгоритму:  
   a. увеличить переменную net_read_timeout сначала до 60 сек, в потом (опытным путем) возможно и больше.  
       net_read_timeout - количество секунд, в течение которых необходимо дождаться дополнительных данных от соединения, прежде чем прервать чтение.  
   b. если вдруг что-то наложилось в это время в нашей сети и сеть стала хуже работать то нужно увеличить параметр connect_timeout например до 10 сек.  
   c. еще вариант проблемы со значениями BLOB, превышающими max_allowed_packet (с дефолтным значением 64MB), что может вызвать эту ошибку у некоторых клиентов (мало вероятно конечно, это нужно чтоб в гис систему записывали например куски растровых карт :)).  
Скорее всего проблема с более длительным выполнение запроса на стороне сервера (вследствие увеличившегося кол-ва записей) и неверноем параметре net_read_timeout.  
  
4.  
Когда памяти остается недостаточно (причем как RAM, так и на диске), OOM killer вынужденно убивает некий процесс (между обрушить всю систему или завершить "жирный" по памяти процесс, выбирается конечно последнее, и часто это именно PostgreSQL как основной потребитель памяти). Для того что бы убедиться что проблема именно в этом, можно посмотреть текущее состояние памяти в конфигах PostgreSQL, свободного места на дисках и временно (только временно) отключить omm-killer.   
По правильному же нужно:  
- проверить настройки памяти для PostgreSQL (shared_buffers, temp_buffers, work_mem, maintenance_work_mem, );  
- возможно добавить/выделить больше памяти серверу с PostgreSQL;  
- в переменной vm.overcommit_memory выставить значение 2 (ядро не будет резервировать больше памяти, чем указано в параметре overcommit_ratio), это не гарантирует, что OOM-Killer не придется вмешиваться, но снизит вероятность принудительного завершения процесса PostgreSQL.  
